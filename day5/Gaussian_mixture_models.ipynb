{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gaussian_mixture_models.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjvakili/MLcourse/blob/master/day5/Gaussian_mixture_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxvVSkKWpeof",
        "colab_type": "text"
      },
      "source": [
        "#Gaussian mixture model\n",
        "\n",
        "Let’s imagine a random variable $x \\in R^{D}$ ($x$ lives in a $D$-dimensional Eucleadian space.) $x$ is said to be drawn from a mixture of Gaussians if:\n",
        "\n",
        "$p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)$,\n",
        "\n",
        "Where $\\mu_k$ is a $D$-dimensional vector, $\\Sigma_k$ is a $D\\times D$ matrix, $\\pi_k$ is a positive scalar such that $\\sum_{k=1}^{K} \\pi_{k} = 1$ and $k = 1,...,K$.  \n",
        "\n",
        "Intuitively speaking, one can think of such model K clusters, each following a Gaussian distribution. The mixture coefficients $\\{\\pi_k\\}_{k=1}^{K}$ are the cluster prior probabilities, and hence they sum to unity. The probability that a given data point $\\tilde{x}$ belongs to a cluster cluster $\\tilde{k}$ is: \n",
        "\n",
        "$p(cluster = \\tilde{k}| x)$\n",
        " $= \\frac{\\pi_\\tilde{k} \\mathcal{N}(x|\\mu_\\tilde{k}, \\Sigma_\\tilde{k})}{\\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}$\n",
        "\n",
        "\n",
        "So unlike the K-means algorithms, there is no hard assignment of clusters to the data-points and the cluster assignment is probabilistic.\n",
        "This is shown in the following example:\n",
        "\n",
        "Let’s consider an iid dataset : $\\{x_n\\}_{n=1}^{N}$:\n",
        "\n",
        "$p(\\{x_n\\}_{n=1}^{N} | ...) = \\prod_{n=1}^{N} p(x_n | ….)$\n",
        "\n",
        "Therefore, \n",
        "\n",
        "$p(\\{x_n\\}_{n=1}^{N} | ...) = \\prod_{n=1}^{N} \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n|\\mu_k, \\Sigma_k)$\n",
        "\n",
        "Note that finding the maximum likelihood solution for such a problem is not possible. If any of the Gaussian clusters collapses on a data-point $x_{m}$ (that is $\\mu_{k}$ = $x_{m}$ and $\\Sigma_{k} = 0_{D\\times D}$) then the above probability becomes infinity. \n",
        "\n",
        "In practice, \n",
        "The parameters of the model $\\{\\mu_k , \\Sigma_k, \\pi_k\\}_{k=1}^{K}$ are estimated by the Expectation-Maximization (EM) algorithm. We do not discuss the details of the algorithm in this lecture but you can learn about them in  the following references. \n",
        "\n",
        "\n",
        "#How many mixtures of Gaussians should I be using for my problem?\n",
        "\n",
        "This is another instance of model complexity. Choosing a large number of clusters can yeild a higher likelihood given the training data but at the expense of using an unnecessarily complex model. One way to tackle this issue is to make use of the information criteria. The information criteria encourage higher likelihood of the data but at the same time punishes higher model complexity. \n",
        "\n",
        "Two common choices of IC are alkaik information criteria (AIC) \n",
        "\n",
        "AIC = $-2 \\log \\mathcal{L} + N_{data}$\n",
        "\n",
        "And Bayesian information criteria (BIC)\n",
        "\n",
        "BIC =  $-2 \\log \\mathcal{L} + N_{data} \\log N_{params}$\n"
      ]
    }
  ]
}